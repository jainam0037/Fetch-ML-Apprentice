{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07b4649c",
   "metadata": {},
   "source": [
    "## Fetch ML Apprentice\n",
    "\n",
    "\n",
    "\n",
    "###### Jainam Shah\n",
    "###### Email Id: jainamshah1500@gmail.com\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ded837f",
   "metadata": {},
   "source": [
    "#### Import Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ce2d85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7596b38d",
   "metadata": {},
   "source": [
    "## Task 1:  Sentence Transformer Implementation\n",
    "\n",
    "Implement a sentence transformer model using any deep learning framework of your choice. This model should be able to encode input sentences into fixed-length embeddings. Test your implementation with a few sample sentences and showcase the obtained embeddings. Describe any choices you had to make regarding the model architecture outside of the transformer backbone.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b3ad3d",
   "metadata": {},
   "source": [
    "#### Initialize the Transformer Model\n",
    "\n",
    "We will use the BERT model as our transformer backbone. We will also use the corresponding tokenizer to preprocess our input sentences. <br/>\n",
    "Code to Initialize the Model and Tokenizer:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2a148d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24eb6203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained BERT model\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1eb6a7fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure the model is set to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "521e1f26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84361eb7",
   "metadata": {},
   "source": [
    "## Encode Sentences\n",
    "\n",
    "Tokenize the input sentences <br/>\n",
    "Pass the tokenized inputs through the transformer model.<br/>\n",
    "Extract and process the embeddings.<br/>\n",
    "\n",
    "\n",
    "Tokenization: Convert the sentences into token IDs that the model can process.<br/>\n",
    "Padding: Ensure that all input sentences are of the same length by padding shorter sentences.<br/>\n",
    "Create Attention Masks: These masks identify the actual tokens vs. the padded tokens.<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bd50111",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentences(sentences, tokenizer, model, device):\n",
    "    # Tokenize the sentences\n",
    "    inputs = tokenizer(sentences, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "    \n",
    "    # Move inputs to the appropriate device\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "    \n",
    "    # Get the model outputs\n",
    "    with torch.no_grad():  # Disable gradient calculation for inference\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    # Use mean pooling to obtain a single vector representation for each sentence\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87762eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample sentence for testing the implementation\n",
    "sentences = [\"Hi I am a test sentence.\", \"Yet another sentence for testing with Jainam.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f0e35e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi I am a test sentence.', 'Yet another sentence for testing with Jainam.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52f9a464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the sentences\n",
    "embeddings = encode_sentences(sentences, tokenizer, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28c92692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0596,  0.0481,  0.2742,  ...,  0.0055,  0.1021, -0.0095],\n",
       "        [ 0.3788, -0.0257, -0.2359,  ...,  0.1561, -0.1965, -0.3291]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba3816b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0596,  0.0481,  0.2742,  ...,  0.0055,  0.1021, -0.0095],\n",
      "        [ 0.3788, -0.0257, -0.2359,  ...,  0.1561, -0.1965, -0.3291]])\n"
     ]
    }
   ],
   "source": [
    "# Print the embeddings\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03440e7",
   "metadata": {},
   "source": [
    "# Test the Implementation\n",
    "Encode a few sentences and display them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0cc69f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample sentences for testing\n",
    "sample_sentences = [\n",
    "    \"You, me, or nobody is gonna hit as hard as life.\",\n",
    "    \"Artificial intelligence is transforming the world.\",\n",
    "    \"Now if you know what you're worth then go out and get what you're worth.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "716c70cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['You, me, or nobody is gonna hit as hard as life.',\n",
       " 'Artificial intelligence is transforming the world.',\n",
       " \"Now if you know what you're worth then go out and get what you're worth.\"]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e50aef4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the sample sentences\n",
    "sample_embeddings = encode_sentences(sample_sentences, tokenizer, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af7b0776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: You, me, or nobody is gonna hit as hard as life.\n",
      "Embedding: tensor([ 2.2176e-01,  2.2432e-01,  3.0695e-01,  1.1605e-01, -1.1473e-01,\n",
      "        -6.4779e-02,  4.6980e-01,  8.0019e-01, -3.6654e-01, -3.0796e-01,\n",
      "         5.0026e-02, -4.6094e-02, -1.4343e-01,  3.5418e-01, -1.2534e-01,\n",
      "         4.2917e-01,  8.6256e-02, -1.3424e-01, -1.3792e-02,  6.0837e-01,\n",
      "        -1.1695e-01,  3.4180e-01, -1.1920e-01, -2.2651e-02,  1.7207e-01,\n",
      "        -5.4144e-02, -1.5742e-01, -1.4741e-02, -3.3203e-01, -1.9860e-01,\n",
      "         1.6375e-01, -2.3867e-01, -2.0066e-01, -1.5885e-01,  2.5050e-01,\n",
      "         2.3776e-01,  2.5012e-02, -1.1694e-01,  1.4643e-01, -1.6711e-01,\n",
      "        -2.9388e-01, -1.9025e-01, -1.9884e-01,  6.5989e-03, -7.1497e-02,\n",
      "        -4.7734e-01,  1.9187e-01,  1.7124e-01,  3.5785e-02, -1.8495e-01,\n",
      "        -1.5227e-01,  8.4010e-02, -3.5321e-01, -2.4385e-01,  2.6484e-02,\n",
      "         1.0005e-02,  2.6120e-01, -5.8567e-01, -3.4005e-01,  3.7883e-01,\n",
      "         4.8994e-03,  1.0925e-01,  1.9755e-01, -3.1815e-01,  6.5609e-02,\n",
      "         5.1729e-01, -3.6785e-02,  2.8555e-01, -7.4723e-01, -4.2971e-01,\n",
      "        -6.3303e-01, -4.2455e-01, -2.5263e-02, -4.0746e-03,  1.1886e-01,\n",
      "         1.4063e-02, -1.6719e-01,  1.0224e-01,  2.4543e-01,  1.7121e-01,\n",
      "         7.9118e-02,  2.0233e-01,  5.9933e-02,  2.0676e-01, -3.0023e-01,\n",
      "         2.0339e-01, -3.8344e-02,  2.9793e-01, -8.9127e-02,  7.7371e-01,\n",
      "         3.2742e-02,  1.0622e-02,  2.1590e-01,  3.5675e-01,  7.3864e-02,\n",
      "        -2.5615e-01,  3.1706e-01,  2.5255e-01, -3.7720e-02,  1.6418e-01,\n",
      "        -6.7579e-02, -5.1446e-01,  2.4681e-01, -3.2512e-01, -1.8793e-01,\n",
      "        -1.5778e-01, -1.0645e-01, -2.1906e-01,  1.6229e-01,  2.1548e-02,\n",
      "         2.5512e-01, -4.2156e-01,  1.8481e-01, -2.3124e-01, -2.3414e-01,\n",
      "        -2.5227e-01,  1.8770e-01, -2.3312e-02,  1.4156e-01, -2.6349e-01,\n",
      "        -7.5293e-02, -2.3184e-01,  3.0173e-01,  6.0870e-01,  1.5498e-01,\n",
      "        -2.3421e-01, -3.0824e-01,  3.5563e-01,  2.8714e-01,  1.7756e-02,\n",
      "         9.5014e-02,  4.6477e-02,  1.1658e-01, -1.2098e-01, -9.1958e-02,\n",
      "         2.1828e-01,  2.3875e-01,  5.3919e-02, -1.4029e-01, -1.5360e-01,\n",
      "         1.0762e-02, -2.0564e-01,  1.1707e-01,  5.5522e-02,  2.5222e-01,\n",
      "        -1.4551e-02, -1.9067e-01, -2.7075e-01,  4.1972e-01, -1.4603e-01,\n",
      "         1.4556e-01,  1.0723e-01, -5.7413e-02, -2.9705e-01, -2.5968e-01,\n",
      "        -7.0937e-03, -6.4881e-01,  1.6918e-01,  1.7975e-01,  4.8183e-01,\n",
      "         2.5035e-01, -5.6357e-01,  3.5566e-01,  8.9378e-02, -1.8755e-01,\n",
      "         9.9293e-02,  4.4919e-02,  2.6595e-01, -2.5223e-01, -3.6767e-01,\n",
      "        -8.4610e-02, -1.0091e-01,  6.1795e-01,  1.2961e-01, -6.1070e-02,\n",
      "        -2.5928e-01,  2.2505e-01,  7.0259e-02, -2.1257e-02, -1.3524e-01,\n",
      "        -1.1760e+00,  3.2067e-01,  4.5455e-01, -1.0440e-01,  6.7708e-02,\n",
      "        -7.5327e-02,  2.7309e-01, -2.4996e-01, -2.1574e-01,  2.0963e-01,\n",
      "        -9.6252e-02,  4.0561e-02,  1.8299e-01,  1.8619e-01,  3.0275e-01,\n",
      "        -8.0484e-01, -2.1231e-01, -3.8799e-01, -3.7778e-01,  2.6574e-01,\n",
      "        -3.2007e-02, -1.1814e-01, -2.9053e-01,  2.1908e-01, -3.5340e-01,\n",
      "         2.3360e-01,  4.6847e-01,  3.0039e-01,  1.3264e-01,  1.0019e-01,\n",
      "        -1.5400e-01,  3.2826e-01, -3.8498e-01,  1.2417e-01, -1.6479e-01,\n",
      "        -3.0767e-01, -7.1029e-02, -3.1776e-02,  9.6991e-02, -1.9631e-01,\n",
      "         1.4188e-01, -1.2580e-01, -2.3966e-01, -3.0107e-02, -9.2168e-02,\n",
      "         4.2594e-01, -8.1381e-02,  1.1524e-01,  8.6758e-02, -2.0456e-01,\n",
      "         2.5628e-01, -3.7536e-01,  4.9479e-04, -2.0736e-02,  3.9823e-02,\n",
      "         6.3475e-01,  9.9285e-02, -4.6644e-01,  1.0793e-01, -4.1232e-01,\n",
      "        -6.5132e-01,  8.6821e-02,  4.3832e-02,  1.2344e-01, -2.1922e-02,\n",
      "         1.5273e-01, -2.2233e-01, -4.4032e-01, -3.2497e-01, -3.8786e-02,\n",
      "        -3.5298e-01, -5.4459e-01, -9.0608e-02, -7.9190e-02, -2.1305e-01,\n",
      "        -3.4274e-01, -3.3815e-01,  3.1877e-01,  1.1584e-01, -2.8326e-01,\n",
      "         1.7570e-01, -1.0149e-01,  1.3389e-01, -1.3011e-01, -5.2968e-01,\n",
      "        -4.5085e-01, -2.8504e-01,  5.5639e-01,  1.9169e-01,  2.8492e-01,\n",
      "         2.1509e-01, -1.8424e-02,  1.9138e-01,  1.6465e-01, -1.4945e-01,\n",
      "        -1.1601e-02,  3.6448e-01,  4.1596e-01, -1.6777e-01, -2.0896e-01,\n",
      "         1.9265e-01,  4.9662e-01, -4.8707e-01,  7.4313e-02,  9.3915e-02,\n",
      "        -1.7823e-01,  4.6588e-01,  1.0411e-01, -1.9322e-01, -4.6735e-02,\n",
      "         4.3686e-03, -3.8760e-01, -2.7802e-01, -2.3537e-01,  5.0293e-01,\n",
      "        -3.4641e-01,  3.6698e-01, -2.5616e-02,  3.5229e-01, -9.2497e-02,\n",
      "         6.9979e-02, -2.7811e-02,  1.3174e-01,  3.8227e-02, -2.4953e-01,\n",
      "         3.2589e-01, -5.3640e-02, -3.6276e-01, -3.6392e+00,  2.6239e-01,\n",
      "         9.1033e-03, -2.1080e-01,  3.9862e-01, -2.3906e-01,  3.9547e-01,\n",
      "        -3.5374e-02, -5.7579e-01,  2.9177e-01,  4.3614e-03, -4.2846e-01,\n",
      "         3.4231e-01, -5.8062e-02,  1.8140e-01, -5.2792e-03,  1.8484e-01,\n",
      "        -2.6492e-01, -3.4403e-01,  4.1673e-01,  1.8362e-01, -2.3424e-01,\n",
      "         1.1209e-01, -2.2456e-01,  2.4777e-01,  6.5700e-01,  2.8692e-01,\n",
      "         9.0429e-02, -2.3282e-02,  1.8968e-01, -4.2313e-01, -2.5162e-01,\n",
      "        -1.8973e-01, -3.8062e-01,  1.4148e-01, -1.0357e-01, -5.9973e-02,\n",
      "        -1.1420e-01, -1.3021e-01, -1.7701e-01, -2.0631e-02, -5.3216e-01,\n",
      "        -4.9230e-01, -2.2352e-01,  3.7647e-01, -1.4418e-01,  5.0250e-03,\n",
      "        -3.1270e-01,  3.3143e-02,  2.5729e-01,  2.0512e-01, -1.7390e-01,\n",
      "         2.8936e-02, -2.0947e-01,  1.9795e-01, -2.3406e-02,  3.3743e-01,\n",
      "         4.6189e-01, -2.6597e-01, -2.0588e-01,  1.5798e-01, -2.9460e-02,\n",
      "        -7.1343e-02,  3.8918e-01,  5.7271e-02, -2.7922e-01, -4.8061e-01,\n",
      "        -5.0569e-01,  2.9179e-01,  9.6198e-02, -2.4192e-01,  1.4428e-01,\n",
      "        -2.9338e-01, -7.2401e-01, -4.7803e-01,  2.1378e-01,  1.1260e-01,\n",
      "        -2.3727e-01, -1.5323e-01, -3.2951e-02,  1.2721e-01, -1.3503e-01,\n",
      "        -8.9542e-02, -1.1150e-01, -4.8949e-02, -2.7496e-01,  3.1835e-01,\n",
      "         9.9551e-02, -6.5715e-02, -1.3386e-02,  4.8968e-01,  3.5362e-01,\n",
      "         4.0800e-01, -1.3177e-01,  4.3191e-01,  4.9569e-01,  6.1626e-01,\n",
      "         1.1956e-01, -1.4099e-01, -3.0035e-01,  1.0771e-01,  1.1525e-02,\n",
      "         4.4694e-01, -6.2400e-02,  7.3607e-02, -1.4596e-01, -6.1024e-01,\n",
      "         1.9723e-01, -1.5936e-01, -1.6266e-01,  2.3577e-01, -2.3753e-01,\n",
      "         4.4378e-01, -5.5485e-01, -1.9386e-01, -7.5161e-01, -6.2871e-02,\n",
      "         8.1705e-01,  2.7512e-02, -7.5883e-02,  6.6085e-02,  2.4484e-01,\n",
      "        -5.3917e-01, -2.4478e-01, -5.2728e-01,  3.6855e-02, -2.7243e-02,\n",
      "        -1.7089e-01, -2.9207e-02, -1.5553e-01, -7.1668e-02,  2.1458e-01,\n",
      "        -3.4942e-01,  9.7369e-02, -2.7800e-01,  1.4963e-01, -4.0472e-01,\n",
      "        -3.0714e-01, -3.0798e-01,  9.6891e-02,  4.1870e-02,  3.5459e-01,\n",
      "        -4.3544e-02, -7.5250e-02, -2.4868e-01,  4.0233e-01, -2.0441e-01,\n",
      "         3.0260e-02, -8.2981e-02,  3.6376e-01, -2.5445e-01,  1.0842e-01,\n",
      "         2.4735e-02, -3.4824e-01, -4.6095e-02,  2.2105e-01,  5.1995e-03,\n",
      "         7.1795e-02,  1.3807e-01, -4.8460e-01, -6.4510e-02,  1.4967e-01,\n",
      "        -9.1190e-02,  8.0528e-02,  9.9325e-02,  4.7241e-01,  3.7722e-01,\n",
      "        -2.5044e-01, -3.2913e-01, -1.3492e-01,  3.0963e-01,  2.5904e-02,\n",
      "         1.7243e-01, -3.1900e-01, -2.2672e-01,  4.2903e-01, -1.1196e-01,\n",
      "        -5.1409e-01,  5.2158e-01, -3.1771e-03,  1.5753e-01, -2.4128e-01,\n",
      "        -7.6036e-02, -9.0624e-02,  6.8963e-02,  1.0046e-01, -7.7396e-02,\n",
      "        -4.4442e-01, -2.2263e-01,  3.3581e-01, -1.2683e-01,  2.6465e-01,\n",
      "        -2.3726e-01, -1.6449e-01, -7.4428e-01, -3.6476e-01,  2.9320e-01,\n",
      "        -2.0832e-01,  3.0881e-01,  4.3944e-01,  1.5504e-01,  1.9744e-01,\n",
      "        -9.5761e-02, -1.3246e-01,  2.5687e-01, -3.6126e-01,  1.2740e-01,\n",
      "         3.5861e-01,  1.5470e-01,  3.0220e-01, -4.3152e-02, -3.6428e-01,\n",
      "        -2.4333e-01,  2.4204e-01,  3.8224e-01, -4.5355e-01,  5.4961e-01,\n",
      "        -1.1100e-01, -3.0622e-01, -2.5437e-01, -2.6962e-01, -2.2152e-01,\n",
      "        -1.3876e-01,  1.3526e-01, -4.3411e-01,  2.9237e-01, -5.6907e-01,\n",
      "        -2.3963e-02,  1.3981e-01, -3.6951e-02, -1.2180e-01,  7.8218e-02,\n",
      "        -1.4281e-01,  3.7954e-02,  5.0921e-02,  1.0412e-01, -3.1306e-01,\n",
      "         3.0369e-01, -1.5015e-02,  4.0108e-02,  3.3285e-02,  1.2286e-01,\n",
      "         2.1819e-02, -4.0599e-01,  1.7710e-01, -3.4780e-01,  1.8490e-01,\n",
      "         2.8377e-01, -1.7296e-02, -7.4902e-02, -1.7171e-01,  3.7869e-02,\n",
      "        -3.4278e-01, -5.6194e-01,  3.6737e-01, -8.9033e-01, -5.8033e-01,\n",
      "         1.8700e-01,  3.3664e-01, -8.6775e-02, -2.9951e-01,  1.7152e-01,\n",
      "         2.2687e-01, -1.8072e-02,  2.9510e-01,  1.2642e-01, -5.5969e-03,\n",
      "         1.3733e-01,  5.7640e-02, -4.3021e-02, -1.3995e-01,  3.9955e-02,\n",
      "         3.3231e-01, -2.2847e-01, -4.4960e-01,  1.1745e-01, -3.6914e-01,\n",
      "        -4.3459e-02, -7.3796e-02, -3.6361e-01, -1.8848e-01,  1.4335e-03,\n",
      "        -3.9544e-01, -4.0194e-01,  2.4624e-01, -2.8329e-01,  9.4994e-02,\n",
      "         4.3892e-01, -1.2014e-01,  1.0553e-01,  1.6426e-01,  2.1416e-01,\n",
      "         2.8235e-01,  3.1922e-01,  4.5273e-01,  4.2767e-01,  4.9399e-01,\n",
      "         1.5749e-01,  3.0186e-03,  4.9227e-01,  1.0552e-01,  5.2525e-01,\n",
      "        -2.7184e-01,  7.7775e-02,  1.5347e-01,  6.0585e-02, -3.7649e-03,\n",
      "        -3.8427e-01,  1.4251e-01,  6.2930e-01, -4.8807e-01, -2.4460e-01,\n",
      "         4.5846e-01,  1.5807e-01, -5.8151e-01, -4.1798e-03,  4.4601e-01,\n",
      "        -5.4409e-02, -1.7640e-01, -7.3773e-02, -1.0323e-02, -3.2755e-01,\n",
      "         6.2795e-01,  1.2837e-01,  7.3360e-02,  1.8127e-01, -5.2891e-01,\n",
      "         2.4351e-01,  3.3157e-01,  4.4552e-01, -2.1757e-01,  3.2967e-01,\n",
      "        -1.2384e-01,  1.3172e-01, -2.4439e-02, -3.1170e-01,  1.0797e-01,\n",
      "         7.2544e-02,  9.0353e-02,  3.3778e-01,  5.1936e-02,  1.2692e-01,\n",
      "         6.0089e-01,  6.4622e-01,  3.3459e-01,  1.2762e-01, -1.1818e-01,\n",
      "         2.9899e-02,  6.7667e-01, -9.1980e-02,  2.1746e-01, -9.4585e-03,\n",
      "         2.0667e-02, -4.1681e-02,  3.5083e-01,  2.8809e-02,  1.2038e-01,\n",
      "         7.5982e-02, -3.1202e-01,  3.7516e-01,  2.0146e-01,  2.6830e-02,\n",
      "         2.9505e-01, -2.2955e-01, -5.4155e-02,  5.1140e-02,  2.7633e-01,\n",
      "         2.6121e-02, -2.1209e-01,  2.4524e-01, -3.7238e-01,  1.3146e-01,\n",
      "        -1.9298e-01,  3.7595e-01, -2.0682e-02,  4.7752e-01,  1.0064e-01,\n",
      "        -8.8840e-02, -4.4219e-01,  2.2382e-01, -1.4899e-01,  1.8401e-01,\n",
      "        -4.9495e-01, -4.8784e-01,  4.7114e-02, -5.1268e-01, -3.8372e-02,\n",
      "        -1.2094e-01,  1.0359e-01, -5.0940e-02,  7.4795e-02, -3.6110e-01,\n",
      "        -6.3538e-01, -3.8310e-01,  3.4855e-01, -3.3399e-01,  4.5104e-01,\n",
      "         2.9668e-01,  5.1687e-01,  1.9165e-01,  5.3729e-01, -6.6555e-02,\n",
      "        -3.4891e-02,  3.7327e-01, -1.5315e-02,  4.9369e-01, -8.0265e-02,\n",
      "         4.6404e-02, -2.9384e-01,  1.2063e-01,  6.6676e-02,  2.6006e-01,\n",
      "        -5.5631e-01, -4.5798e-02, -1.0888e-01, -1.7281e-01,  3.7082e-02,\n",
      "        -3.4029e-01, -3.3294e-01, -6.8479e-02, -2.2418e-01, -2.3993e-01,\n",
      "        -2.0713e-01,  3.7477e-01, -2.6179e-01,  9.4677e-02, -1.3999e-02,\n",
      "         2.6655e-01, -2.8324e-01,  5.8421e-03,  1.1381e-01,  2.5737e-01,\n",
      "         3.7094e-02, -3.4670e-01,  2.4639e-01, -3.5137e-01, -3.5780e-01,\n",
      "        -1.6997e-01, -7.1655e-01, -1.9758e-01,  3.1203e-01, -9.7559e-02,\n",
      "        -8.9550e-02, -1.3837e-01,  1.7359e-01, -1.5049e-02, -5.2004e-02,\n",
      "        -3.0012e-01,  4.2275e-03,  5.7481e-02, -3.2109e-01, -1.4491e-01,\n",
      "         1.1029e-01,  2.4071e-02,  2.1181e-01,  1.3109e-01, -4.3870e-02,\n",
      "        -1.7581e-01,  3.2934e-02,  5.8931e-03])\n",
      "\n",
      "Sentence: Artificial intelligence is transforming the world.\n",
      "Embedding: tensor([ 3.7276e-01,  1.3105e-02,  1.1224e-01,  1.8695e-01,  2.3877e-01,\n",
      "        -3.1953e-01,  1.4680e-01,  6.6675e-01, -3.3926e-03, -6.4815e-01,\n",
      "         2.3874e-01, -8.8737e-02, -5.1851e-01,  3.1081e-01, -1.8771e-01,\n",
      "        -1.2120e-01,  1.5992e-01,  2.4711e-01, -1.7498e-01,  5.4544e-01,\n",
      "        -3.6782e-01, -3.9224e-02, -8.6282e-04,  3.7265e-01,  6.7596e-02,\n",
      "        -3.3109e-01, -3.2846e-01,  3.7137e-02, -4.0409e-01,  1.6900e-01,\n",
      "         2.6232e-02, -1.1103e-01, -3.1458e-01, -6.9656e-02,  7.3375e-02,\n",
      "        -1.7732e-01, -4.2796e-01, -3.3516e-03, -5.8570e-01,  2.5429e-01,\n",
      "        -4.7379e-01,  1.0206e-01, -1.7349e-01,  2.0811e-02, -4.4026e-01,\n",
      "        -1.7654e-01, -2.0242e-01,  3.9026e-01,  1.9986e-01, -2.1837e-01,\n",
      "        -4.0346e-01, -6.9021e-02, -1.0235e-02,  6.9698e-02, -9.6978e-02,\n",
      "         2.9049e-01,  1.9612e-01, -2.7668e-01, -4.8666e-01, -5.9027e-03,\n",
      "         2.2299e-01, -6.8704e-02, -1.2062e-01, -2.9505e-01, -3.4853e-01,\n",
      "        -1.9642e-02,  2.5866e-01,  2.6933e-01, -4.6819e-01, -3.4605e-01,\n",
      "        -8.3491e-02, -3.0275e-01, -2.7867e-01, -2.6580e-01, -3.7342e-01,\n",
      "         7.2434e-02, -2.6030e-01,  6.6160e-02,  3.1762e-01, -3.1930e-01,\n",
      "        -2.6317e-01,  6.2078e-01, -4.1945e-02,  2.5632e-01,  8.7843e-02,\n",
      "         3.0036e-02,  2.7192e-02,  5.8358e-01, -5.3372e-02,  7.7714e-03,\n",
      "         8.6476e-02, -2.7414e-01,  1.7241e-01,  4.6933e-01,  2.4082e-01,\n",
      "         8.9487e-02, -1.6408e-01, -1.4442e-01, -2.8734e-01,  4.7345e-02,\n",
      "         4.3194e-01, -3.8482e-01,  3.3247e-01, -4.8720e-02,  7.3598e-02,\n",
      "        -9.1408e-02,  2.3068e-01, -1.1961e-01,  2.2390e-01, -3.0275e-01,\n",
      "         1.0775e-01, -3.1275e-02,  4.2691e-02, -1.2416e-01, -3.6390e-01,\n",
      "         3.8502e-01,  1.6526e-01, -2.0091e-01,  4.4796e-01,  2.0407e-02,\n",
      "         1.1279e-01,  1.0739e-01, -2.0492e-01,  6.7185e-01, -2.8314e-01,\n",
      "         1.3057e-01, -7.0394e-02, -1.1605e-01, -3.4050e-02, -3.7298e-01,\n",
      "         5.6026e-01,  4.7699e-01,  7.3866e-02,  8.4970e-02,  1.5692e-01,\n",
      "         3.2517e-01,  2.7933e-01,  8.6544e-02,  6.3372e-02, -1.2846e-01,\n",
      "         4.9792e-01, -1.5110e-01,  2.4264e-01, -2.8040e-02,  1.1516e-01,\n",
      "        -1.0002e-01,  2.6331e-02, -2.7057e-01,  6.3496e-02, -1.8618e-01,\n",
      "        -1.4930e-01,  1.0737e-01, -4.1568e-01, -4.4221e-01,  4.4447e-01,\n",
      "        -2.0629e-01, -3.4726e-01,  2.9260e-01,  1.2335e-01,  2.2154e-01,\n",
      "         9.0359e-02, -2.3654e-01, -2.5301e-02,  2.7799e-02, -2.2561e-01,\n",
      "         1.6976e-02,  3.8024e-01,  6.4336e-01, -1.1376e-02,  1.4346e-01,\n",
      "        -3.4469e-01,  2.7699e-02,  8.7993e-01,  1.6450e-01, -7.1909e-03,\n",
      "         3.3156e-02,  1.1111e-01,  2.3178e-01,  8.5887e-02,  3.9075e-01,\n",
      "        -6.7735e-01,  2.4606e-02,  8.0233e-02, -1.7063e-01,  3.9922e-01,\n",
      "        -4.0178e-01,  3.1534e-01, -1.7968e-01,  6.9532e-02,  1.2211e-01,\n",
      "         5.8057e-02, -1.4295e-01, -5.2024e-02,  9.9195e-03,  2.0884e-01,\n",
      "        -4.4088e-01,  2.2316e-01,  1.2726e-01,  7.6932e-02,  2.5309e-03,\n",
      "        -4.5298e-02, -9.7372e-02,  3.1139e-01, -4.9087e-02,  2.5160e-02,\n",
      "        -2.2134e-01,  2.4815e-01, -1.3911e-01, -2.3528e-01,  5.7611e-02,\n",
      "        -3.7329e-01,  1.5387e-01, -2.7111e-01,  1.6143e-01, -7.7984e-04,\n",
      "         5.2659e-02, -2.4111e-01,  1.5415e-01, -5.0470e-01, -7.2461e-02,\n",
      "         3.8887e-02, -2.6973e-01, -1.4108e-01,  3.1913e-01,  1.9902e-01,\n",
      "         6.1400e-01,  5.3688e-01,  2.2975e-01,  4.6600e-01,  1.5196e-01,\n",
      "        -1.2891e-01, -1.7993e-01,  6.5683e-02, -1.2655e-01, -1.9219e-01,\n",
      "         2.6142e-02, -3.7056e-01, -4.0260e-01, -5.8475e-02, -1.0861e-01,\n",
      "        -2.2007e-01, -3.0693e-01,  1.1445e-01,  3.5527e-01,  1.4240e-01,\n",
      "        -6.9037e-02,  4.5013e-01, -1.7171e-01,  1.0037e-01, -4.5958e-01,\n",
      "        -5.2176e-01,  1.8109e-02,  2.6376e-01, -1.9214e-01, -1.1140e-01,\n",
      "        -1.7357e-01, -7.9187e-02,  3.3361e-01,  6.0976e-02,  7.9180e-02,\n",
      "         5.7434e-01, -1.9012e-01,  1.9462e-01,  9.6536e-02,  7.4814e-02,\n",
      "        -5.7106e-01,  1.2308e-01, -1.9110e-01,  2.3760e-01, -1.0228e-01,\n",
      "         2.4924e-01, -4.4039e-01,  2.8710e-01,  2.5184e-01, -7.0964e-02,\n",
      "        -6.4227e-01,  9.9894e-02,  7.1919e-02,  2.7457e-02,  3.9789e-02,\n",
      "        -1.1093e-01,  5.6045e-01, -1.8560e-01, -1.1398e-01,  1.2872e-01,\n",
      "        -3.6296e-01, -1.6678e-01, -5.2417e-02, -5.4957e-03, -7.5194e-02,\n",
      "        -1.7375e-01,  1.8950e-01, -3.5194e-01, -2.9331e-01,  1.5390e-01,\n",
      "         2.7485e-02,  1.6542e-01, -3.2148e-01,  2.0997e-02,  1.4180e-01,\n",
      "        -2.3796e-01,  2.9860e-02, -2.9119e-03,  8.5472e-02, -2.3799e-01,\n",
      "        -1.6378e-02,  2.6957e-01, -7.8155e-02, -4.8166e+00, -6.6687e-02,\n",
      "         3.9910e-01, -1.9373e-01, -2.8719e-01, -7.8990e-03, -9.7094e-02,\n",
      "         1.4467e-01, -4.1743e-01, -2.2492e-01,  1.1335e-01,  4.7796e-02,\n",
      "         3.9946e-01,  2.1645e-01, -7.6091e-02,  2.4220e-02,  4.6502e-01,\n",
      "        -1.5705e-01, -4.9280e-01,  4.8263e-01, -8.6189e-02, -4.6818e-01,\n",
      "         1.5145e-01, -1.3572e-01,  1.9497e-01,  1.0820e-02, -1.3082e-01,\n",
      "        -1.9392e-01, -1.2459e-01, -1.2161e-01, -2.6931e-02, -1.3282e-01,\n",
      "         2.1037e-01,  2.7158e-01,  1.9684e-01,  1.2286e-01, -4.9077e-02,\n",
      "        -1.6310e-01,  3.1540e-01,  1.7314e-02,  2.4938e-01, -2.7648e-02,\n",
      "        -1.5347e-02,  4.9623e-02,  5.5099e-01, -2.9498e-01, -1.6039e-02,\n",
      "         2.1345e-01, -1.2957e-02,  1.6948e-01,  1.8850e-01,  4.6857e-02,\n",
      "        -7.0298e-02, -5.1015e-02, -1.8066e-01,  3.5314e-02,  6.0178e-01,\n",
      "         3.0402e-01,  1.2411e-02, -4.7080e-01,  2.6652e-01, -1.8868e-01,\n",
      "        -1.2916e-01,  3.3241e-01,  5.1870e-02, -3.3255e-01, -1.6294e-01,\n",
      "        -5.5841e-01, -1.1490e-01,  3.6553e-01, -1.1286e-01,  1.7774e-01,\n",
      "        -2.1894e-01, -7.3634e-01, -2.4089e-01, -2.1691e-01, -3.7827e-02,\n",
      "        -1.4649e-01,  3.9957e-01,  6.2789e-02, -1.3720e-01, -3.7897e-01,\n",
      "        -1.5718e-01, -2.8481e-01,  2.9260e-01, -5.7058e-01,  2.3551e-02,\n",
      "         1.6908e-01,  3.6534e-02,  2.0752e-01,  4.2599e-01,  1.8862e-03,\n",
      "         3.8890e-01,  2.8159e-02,  7.4520e-01, -2.0908e-01,  1.6808e-01,\n",
      "         9.6406e-02, -3.3511e-02,  1.3407e-01, -1.3475e-01, -2.1234e-01,\n",
      "         5.2554e-02, -1.2531e-01,  1.3916e-01, -4.2864e-02, -2.3180e-01,\n",
      "        -5.9169e-03,  1.5258e-01, -9.0752e-02, -1.1634e-01, -1.1067e-01,\n",
      "         5.7279e-01,  6.7808e-02, -7.1758e-02, -1.1824e-01, -1.7790e-03,\n",
      "         4.7813e-01, -1.7287e-02, -1.9502e-01,  8.9844e-03,  6.6944e-01,\n",
      "        -1.1778e-01,  2.0679e-01, -4.0381e-03,  8.7713e-02,  3.5426e-02,\n",
      "        -1.1476e-01, -1.3412e-02, -2.6912e-01, -1.8877e-01,  2.0202e-01,\n",
      "        -1.2023e-01, -5.9485e-03,  9.9993e-02,  3.3018e-01,  6.7571e-02,\n",
      "         2.4038e-01, -8.6151e-02,  4.1278e-01,  3.2450e-01,  2.6583e-01,\n",
      "         2.7970e-01, -3.4212e-01, -3.0835e-01, -1.5145e-02,  1.1154e-01,\n",
      "         1.6259e-01,  3.5228e-01,  2.8350e-01, -3.5627e-01, -2.2416e-01,\n",
      "        -9.2621e-03, -1.0015e-02,  1.4732e-01, -2.2472e-01,  3.6710e-01,\n",
      "        -1.1864e-01, -2.5842e-01, -5.6254e-01, -1.3851e-01,  2.6528e-01,\n",
      "        -3.2880e-01,  2.2319e-01,  3.4515e-01,  7.1748e-01,  3.3280e-02,\n",
      "        -3.9684e-01, -4.4344e-01,  2.5965e-02,  1.5779e-01, -8.7351e-03,\n",
      "        -2.0315e-01, -1.4232e-01, -3.4460e-01,  7.6953e-02,  1.8480e-01,\n",
      "        -3.7259e-01, -6.9120e-01, -1.1628e-02,  1.1160e-01, -1.0315e-01,\n",
      "        -4.0581e-01, -2.1468e-01,  2.2712e-01,  2.5094e-01, -5.4550e-02,\n",
      "        -2.2030e-01,  1.2616e-01,  5.2580e-02,  8.0845e-02,  1.7797e-01,\n",
      "        -1.8960e-01, -3.5922e-01, -3.6857e-01, -6.5724e-03, -1.2614e-02,\n",
      "        -1.8995e-01,  3.2743e-01,  1.3765e-01,  3.6414e-01,  3.8325e-01,\n",
      "        -3.2053e-02, -1.3659e-01,  1.1863e-01,  4.7121e-03, -2.3594e-01,\n",
      "         1.7368e-02, -2.7702e-01,  2.1417e-01, -2.2342e-01, -9.3619e-02,\n",
      "        -1.7337e-01, -1.7922e-01, -1.2677e-03,  3.3599e-01, -2.0519e-03,\n",
      "         1.4065e-01, -6.2774e-01,  1.9034e-01, -4.7014e-01, -4.8840e-02,\n",
      "        -4.7673e-02,  3.2596e-02, -5.1828e-01,  1.7215e-02, -3.4130e-01,\n",
      "         2.4228e-01, -3.4724e-01, -5.4601e-01,  8.9608e-02,  1.9977e-01,\n",
      "        -1.7842e-01,  2.3202e-01,  2.1860e-01,  2.5066e-01, -1.0609e-01,\n",
      "         7.3941e-02, -1.0038e-02, -1.3853e-02, -2.1518e-01, -4.6450e-01,\n",
      "        -4.9551e-01, -3.3902e-02,  8.3041e-02,  7.4623e-02, -2.0344e-01,\n",
      "         6.6470e-01,  5.1059e-02,  7.3120e-02, -2.1688e-01, -9.6120e-02,\n",
      "        -2.4634e-01, -7.0209e-02, -1.6769e-02, -4.9934e-01, -8.0168e-02,\n",
      "        -2.2136e-01, -1.6663e-01, -1.6493e-01, -1.2664e-01, -1.2993e-01,\n",
      "        -3.3977e-01,  2.2781e-02,  5.0629e-02, -2.3326e-02, -2.9700e-01,\n",
      "        -3.5196e-01,  3.9660e-01, -2.4238e-01, -8.0657e-02,  1.0875e-01,\n",
      "        -6.7851e-02,  2.0601e-02,  5.9280e-02, -7.3993e-03, -1.1000e-02,\n",
      "        -2.5332e-01, -2.6794e-01, -3.2405e-01,  2.1956e-01,  5.4804e-01,\n",
      "         1.4742e-01, -4.3265e-01, -2.5915e-01, -2.6227e-01,  2.0149e-01,\n",
      "         2.3704e-01,  5.1948e-02, -9.3810e-02, -2.0139e-01, -2.8245e-01,\n",
      "         6.2212e-02,  4.5973e-01,  1.6493e-01,  6.6469e-02,  1.0393e-01,\n",
      "        -1.5368e-01, -9.8307e-02, -2.4642e-01,  8.3412e-02,  3.9177e-01,\n",
      "         3.2641e-01, -1.2503e-01, -1.2824e-01,  2.7376e-01, -6.7556e-02,\n",
      "         2.7352e-01, -1.6312e-01,  1.4577e-01, -3.4922e-01, -7.4867e-02,\n",
      "         3.8435e-01,  5.0556e-01, -4.5196e-01,  3.1572e-01,  1.8877e-01,\n",
      "        -7.4982e-02, -1.3330e-02,  2.8743e-01, -2.8262e-01, -6.9877e-02,\n",
      "         3.0552e-02, -6.3060e-02,  3.3900e-01,  4.5757e-01, -1.8915e-01,\n",
      "         1.8533e-01, -9.7150e-02,  2.0161e-01, -1.8678e-01,  4.0958e-01,\n",
      "        -3.2193e-02,  5.5591e-01,  1.5786e-01, -3.8316e-01,  1.1371e-01,\n",
      "         2.6793e-01,  2.6842e-01, -6.5543e-02,  5.0472e-01,  2.7332e-01,\n",
      "        -1.3204e-01,  4.0881e-01, -2.2293e-02,  7.5645e-02, -1.8760e-01,\n",
      "         2.5481e-01, -2.2057e-01,  1.7418e-01,  1.4323e-01,  6.6174e-02,\n",
      "        -6.0437e-02, -5.8867e-02,  3.2027e-01, -2.5044e-01,  3.7009e-01,\n",
      "         3.9988e-01,  5.1143e-02,  7.4222e-02, -6.9391e-02,  3.2064e-01,\n",
      "         7.6721e-01, -3.0834e-01,  1.9841e-01,  4.5648e-01,  1.6522e-01,\n",
      "        -3.8002e-01, -7.1059e-03, -1.0906e-01,  1.2269e-01, -1.1327e-01,\n",
      "         1.2927e-01, -8.8018e-02,  1.6188e-01,  7.2417e-01,  7.8196e-02,\n",
      "        -2.7453e-01, -1.4020e-01, -9.9694e-02, -1.5373e-01,  1.6998e-02,\n",
      "        -1.9055e-01, -5.0799e-02,  9.9822e-02, -3.1478e-01, -2.1318e-01,\n",
      "         1.7496e-01,  1.0645e-01, -2.2155e-01, -2.7500e-01, -8.5374e-02,\n",
      "        -8.7489e-02, -3.2464e-01, -5.5064e-02, -7.8961e-02, -6.7602e-02,\n",
      "        -1.2868e-01,  1.9504e-01,  1.3726e-01,  3.1173e-01, -2.2249e-01,\n",
      "        -1.6419e-01,  9.7722e-02, -4.5564e-01, -3.3737e-01, -3.1251e-02,\n",
      "        -1.2956e-01, -1.4348e-01, -3.6973e-01, -1.5712e-02,  5.1392e-03,\n",
      "        -6.5292e-01,  2.4578e-01, -2.4530e-01,  3.5401e-01, -1.4114e-01,\n",
      "         1.7456e-01, -3.9104e-02,  6.0666e-02, -2.8596e-01,  1.3887e-01,\n",
      "         5.6511e-02,  2.1268e-01, -3.4171e-01,  2.6859e-01, -2.7541e-02,\n",
      "         3.0280e-01, -2.2189e-01,  1.3280e-01, -4.4296e-01,  3.5172e-01,\n",
      "        -2.4307e-01, -1.7404e-01,  2.1330e-01, -1.4352e-01, -3.0432e-02,\n",
      "         2.2628e-01, -3.8175e-01, -5.9473e-01,  1.2876e-01,  1.7256e-01,\n",
      "         2.1333e-01, -4.4103e-01, -3.0736e-01, -1.2918e-01, -3.1462e-01,\n",
      "         1.0213e-01,  2.5438e-01, -1.2456e-01, -5.7033e-02,  1.1432e-01,\n",
      "        -1.0904e-02, -1.7830e-01,  2.2258e-01,  8.5076e-02,  4.0997e-01,\n",
      "        -2.8770e-01,  2.4885e-01,  4.0360e-02])\n",
      "\n",
      "Sentence: Now if you know what you're worth then go out and get what you're worth.\n",
      "Embedding: tensor([-5.7732e-02, -7.0887e-02,  1.0494e-01, -2.4922e-01, -1.0867e-01,\n",
      "        -8.2407e-02,  6.0372e-01,  7.3540e-01, -3.0647e-01, -2.8271e-01,\n",
      "         6.6239e-02, -1.3766e-01, -2.3600e-01,  2.6107e-01, -3.7575e-01,\n",
      "         1.8058e-01, -3.8500e-02,  8.5772e-02,  4.7920e-03,  5.9964e-01,\n",
      "        -2.0956e-01,  2.9619e-01, -2.7621e-01,  4.0895e-01,  7.3843e-01,\n",
      "         1.3330e-01, -2.8812e-01,  1.1962e-01, -3.3046e-02, -6.2368e-01,\n",
      "        -8.7585e-03,  3.5001e-01, -2.0415e-01, -1.1862e-01,  8.9641e-02,\n",
      "         1.1567e-01,  2.2945e-01,  1.5837e-03, -3.2638e-01,  1.0902e-01,\n",
      "        -9.5825e-01, -7.0214e-01, -3.7229e-01,  3.4458e-01, -1.5633e-01,\n",
      "        -6.3215e-01,  3.9910e-01, -1.9905e-01,  1.2137e-01,  9.8237e-02,\n",
      "        -2.0846e-01,  6.1885e-01, -9.3919e-01, -2.2613e-01,  3.3867e-01,\n",
      "         7.6744e-01, -4.2437e-02, -5.8402e-01, -1.0041e+00, -1.4798e-01,\n",
      "        -6.2477e-01,  4.2615e-01,  6.8793e-02, -2.8957e-01,  1.2650e-01,\n",
      "         2.0410e-01,  2.9611e-02,  2.9545e-01, -6.1474e-01, -4.6149e-01,\n",
      "        -6.2143e-01, -6.7014e-01, -5.3320e-02, -1.3913e-01, -1.8675e-01,\n",
      "         2.4222e-01, -5.8898e-01,  1.5318e-01, -1.5366e-01,  1.7303e-01,\n",
      "        -2.3347e-02,  4.6146e-01, -4.2543e-01,  3.4855e-01,  2.9209e-01,\n",
      "        -1.7408e-01, -1.1772e-01,  2.5928e-01, -3.3167e-01,  4.7608e-01,\n",
      "        -9.9557e-03, -1.3983e-01,  3.2563e-02,  3.7063e-01,  1.9678e-01,\n",
      "         4.6864e-02,  2.6221e-01,  1.0802e-01, -1.0428e-01,  4.6048e-01,\n",
      "         3.7243e-01, -9.1763e-01,  2.2270e-01, -5.0065e-01, -1.2423e-01,\n",
      "         4.0188e-02, -1.5076e-01, -5.0363e-02,  4.1318e-01,  4.5481e-01,\n",
      "         1.9795e-01, -7.1418e-01, -1.2853e-01, -3.1734e-02, -7.7310e-02,\n",
      "        -2.8148e-01, -1.6308e-01, -4.1566e-01,  3.6823e-02,  1.6691e-01,\n",
      "        -1.1101e-01, -4.9761e-01, -1.1909e-01,  1.0805e+00,  8.1377e-03,\n",
      "        -5.6709e-01,  1.4214e-01,  4.8658e-01,  1.2991e-02, -1.9241e-01,\n",
      "         4.0010e-01,  2.6910e-01,  7.3580e-02, -4.5502e-01, -5.6047e-01,\n",
      "        -4.7859e-02,  3.6208e-01, -1.8218e-01, -4.4114e-01, -7.2680e-02,\n",
      "         2.1605e-01, -2.3243e-01,  7.3467e-01,  9.8766e-03,  3.8964e-01,\n",
      "         5.7383e-02, -2.8151e-01, -3.1316e-01,  7.7810e-01, -3.5332e-02,\n",
      "         4.6399e-01, -1.1065e-01,  3.0089e-01, -5.4207e-01, -6.3909e-02,\n",
      "        -1.7843e-01, -5.5200e-01,  4.2341e-01, -5.8729e-02,  2.6041e-01,\n",
      "         2.8193e-01, -4.8327e-01,  2.0757e-02,  3.1612e-01, -3.8311e-01,\n",
      "         1.0500e-01, -9.3220e-02,  5.9833e-01,  7.5156e-02,  4.0070e-02,\n",
      "        -6.0152e-01, -5.2080e-02,  1.2082e+00,  2.7038e-01, -2.2980e-02,\n",
      "        -5.4762e-01, -4.1434e-01,  7.1702e-01, -2.3518e-02, -1.6834e-01,\n",
      "        -1.5516e+00,  5.8203e-01,  2.5604e-01, -5.2165e-02,  8.4665e-02,\n",
      "         1.8320e-01,  2.8000e-01, -3.4658e-01, -4.4637e-01, -1.5517e-01,\n",
      "         8.5177e-02,  1.5437e-01, -6.4413e-01, -1.6142e-01,  2.0730e-01,\n",
      "        -7.9688e-01, -4.2463e-01, -3.0525e-01, -7.1403e-01,  1.4775e-01,\n",
      "        -2.4596e-01, -1.0442e-01,  1.3371e-01,  4.8045e-01, -3.8672e-01,\n",
      "        -2.3784e-01,  2.9139e-01,  4.8696e-02, -4.3956e-02,  4.3682e-01,\n",
      "        -5.2399e-01,  3.6056e-01, -1.7137e-01,  2.2236e-01, -4.7286e-01,\n",
      "        -3.7307e-02,  1.2841e-01,  3.8949e-02,  5.0142e-02, -3.1607e-01,\n",
      "         1.2755e-01, -2.0124e-01, -3.8631e-01,  1.9870e-01, -2.4337e-01,\n",
      "         3.9761e-01,  1.6081e-01,  1.7918e-01, -2.2066e-01,  2.9171e-01,\n",
      "        -2.3929e-01, -8.0726e-02,  3.4717e-01, -1.9289e-01, -3.6646e-01,\n",
      "         4.4920e-02, -1.5469e-01, -3.0432e-01,  3.1796e-02, -3.4654e-01,\n",
      "        -1.8978e-01,  1.0041e-01,  1.8116e-01,  2.1795e-01, -1.9230e-01,\n",
      "         4.1455e-01, -4.8721e-01, -7.8049e-02,  2.9756e-01, -2.2845e-01,\n",
      "        -3.0328e-01, -6.7622e-01, -4.2648e-01, -2.1429e-01,  2.3318e-01,\n",
      "        -5.2425e-01, -5.4778e-01, -1.3587e-01, -4.6643e-01,  3.3448e-01,\n",
      "         5.5731e-01,  2.7268e-01,  1.6265e-01,  5.4059e-01, -7.0934e-01,\n",
      "        -1.4788e-01,  3.0045e-01,  2.7024e-01,  1.5883e-01,  3.9154e-01,\n",
      "        -7.2984e-02,  2.1847e-01, -1.8362e-01,  7.9908e-01, -3.0232e-01,\n",
      "        -3.4994e-01,  3.6929e-02,  3.4466e-01,  4.8297e-02, -4.0051e-01,\n",
      "         1.6524e-01,  3.5157e-01, -7.6849e-01,  6.2093e-01,  5.0162e-01,\n",
      "        -1.1385e-01,  7.4220e-02,  8.7185e-01, -5.1053e-01, -4.0663e-01,\n",
      "        -3.6692e-02, -3.3996e-02, -4.4074e-01, -2.1459e-01,  7.3133e-01,\n",
      "         7.9549e-02,  1.8245e-01,  3.9155e-01,  5.7666e-01, -2.3268e-01,\n",
      "         3.4046e-02, -6.6623e-01,  4.8380e-01,  3.5200e-01, -6.1109e-01,\n",
      "         4.1121e-01,  1.5548e-01, -8.5649e-01, -1.8071e+00,  2.7957e-02,\n",
      "        -1.6298e-01, -2.9460e-01,  4.8806e-01,  3.5850e-02,  2.3910e-01,\n",
      "        -2.6805e-01, -5.5175e-01,  9.4243e-02, -2.6294e-01, -5.9097e-01,\n",
      "         1.3291e-01,  3.8773e-01,  4.2808e-01,  1.3100e-01,  6.6831e-01,\n",
      "        -2.6289e-01, -7.2660e-01,  7.2439e-01, -1.8690e-01, -1.7628e-01,\n",
      "         1.2586e-01,  1.5206e-01,  3.2380e-01,  7.9790e-01, -5.7098e-02,\n",
      "        -2.6928e-01,  4.9062e-02,  9.8795e-02, -3.1094e-01, -2.2430e-01,\n",
      "        -2.9846e-01, -1.1834e-01,  6.6139e-01, -8.9880e-02, -1.7767e-01,\n",
      "        -3.1328e-01, -1.0166e-01, -2.5371e-01,  8.5530e-03, -2.0139e-01,\n",
      "         8.3159e-02,  2.9782e-01,  1.0388e+00, -8.8854e-02, -3.8000e-01,\n",
      "        -2.3865e-01,  3.2425e-02,  1.9876e-01,  2.0218e-01,  4.7148e-02,\n",
      "         1.9304e-02, -3.6545e-01, -2.4960e-01,  1.6778e-01,  6.0235e-01,\n",
      "         1.7975e-02,  5.3741e-02, -1.5430e-01,  2.7701e-01,  6.1641e-02,\n",
      "        -2.6394e-02,  1.1925e-01,  4.4801e-02, -4.0134e-01, -4.8985e-01,\n",
      "        -2.4343e-01,  8.0521e-02, -1.5260e-01, -1.2678e-01,  4.6710e-01,\n",
      "        -5.3156e-01, -5.9287e-01,  6.4564e-02, -7.1584e-02, -1.8706e-01,\n",
      "         1.8328e-01, -1.1944e-01,  1.0564e-01,  5.3980e-02, -4.8013e-01,\n",
      "        -2.3683e-01, -9.4509e-02, -8.7422e-02, -4.2533e-01,  2.1962e-01,\n",
      "         1.2331e-01, -7.8886e-02, -4.7738e-01,  6.8705e-01,  3.9331e-01,\n",
      "        -4.1374e-03,  4.1624e-01,  2.1149e-01,  4.0025e-02,  6.2488e-01,\n",
      "        -1.4681e-01, -2.6715e-01,  2.9735e-01, -2.5224e-01, -3.3507e-01,\n",
      "         6.7514e-01,  9.8698e-02,  2.6377e-01,  1.8487e-01, -4.0899e-01,\n",
      "        -5.9144e-02,  2.0142e-02,  4.2106e-01,  3.6984e-01, -5.8169e-01,\n",
      "         3.6597e-01,  2.5248e-01, -4.9011e-01, -7.2560e-01, -2.4371e-01,\n",
      "         8.2757e-01, -2.3577e-01,  2.5584e-01,  3.5974e-01,  4.8134e-01,\n",
      "        -3.9967e-01, -4.5950e-01, -5.9611e-01, -1.1220e-01, -1.3701e-01,\n",
      "        -2.2970e-01, -2.8884e-01,  3.7554e-01,  2.9218e-01, -3.6224e-02,\n",
      "        -2.1175e-01,  2.9801e-02, -3.3402e-02,  3.0713e-01,  1.0262e-01,\n",
      "        -5.3588e-01,  2.6427e-01,  5.8374e-02,  1.8198e-02,  4.9772e-01,\n",
      "        -1.8077e-02, -1.5508e-01, -7.3229e-01,  2.3455e-01, -1.4779e-01,\n",
      "        -2.0684e-01, -2.7313e-01,  4.2153e-01,  1.2837e-01, -1.0097e-01,\n",
      "        -2.6131e-01, -6.5998e-01,  1.6154e-01, -3.2561e-01,  1.7610e-01,\n",
      "        -5.2447e-02,  1.1671e-01, -8.4188e-01,  6.1736e-02,  4.9344e-01,\n",
      "        -8.5178e-02,  5.6250e-01,  3.5971e-01,  2.4564e-01,  1.1986e-01,\n",
      "        -2.3085e-01, -2.3455e-01,  4.6413e-01,  1.9530e-01, -3.1547e-01,\n",
      "         4.7114e-01, -5.1967e-01, -3.0412e-01,  5.8295e-01,  3.0284e-01,\n",
      "        -1.2752e-01,  1.7796e-02,  2.4591e-01, -1.6942e-01, -3.5176e-01,\n",
      "        -4.4377e-01, -6.0070e-01, -7.6857e-02,  1.5964e-01, -2.8054e-02,\n",
      "        -5.3641e-01, -1.6573e-01, -1.2822e-02, -5.6314e-01,  6.9238e-01,\n",
      "        -1.2143e-01, -3.8579e-02, -7.3955e-01, -5.6518e-01,  4.7058e-01,\n",
      "        -2.3340e-01,  4.6484e-01,  3.1875e-01,  2.6799e-01, -1.5849e-01,\n",
      "        -2.7062e-01,  3.7212e-02,  3.8146e-02, -9.2147e-02, -4.9548e-02,\n",
      "         1.7826e-01, -1.2683e-01,  5.9164e-01, -4.2814e-01, -1.7497e-01,\n",
      "        -3.4977e-01, -3.2569e-01,  2.4818e-01, -4.6900e-01,  2.6520e-01,\n",
      "        -1.5158e-01, -4.7464e-01, -1.2239e-01, -3.4856e-01, -1.7565e-01,\n",
      "         2.1592e-02,  5.0171e-01, -5.6606e-01, -3.1082e-01, -4.9287e-01,\n",
      "         1.4353e-01, -2.3217e-01, -2.2482e-01, -3.3597e-01, -2.6323e-01,\n",
      "         5.0484e-01,  3.0151e-01,  1.6762e-01,  3.7507e-01, -2.7700e-01,\n",
      "        -1.7812e-01, -6.9677e-02, -1.1471e-01, -5.5694e-01, -4.0188e-02,\n",
      "        -1.2225e-01, -3.9710e-01, -1.8974e-01, -2.7609e-01, -2.5757e-01,\n",
      "         5.8806e-01, -5.0901e-01,  3.5204e-01,  2.0496e-01,  1.5639e-01,\n",
      "        -2.9980e-01, -2.6624e-01, -2.1974e-01, -8.0121e-01, -6.6065e-01,\n",
      "         2.0947e-01,  3.3249e-01,  2.5918e-01,  5.3522e-01,  1.3488e-01,\n",
      "         3.4258e-01,  5.7801e-02,  4.1963e-02,  1.2370e-01,  3.3966e-01,\n",
      "         4.6918e-02, -4.3045e-01, -4.6619e-01, -8.8153e-02, -1.4081e-01,\n",
      "         9.4117e-02, -1.5956e-01,  1.1667e-01,  2.1063e-01, -1.4216e-01,\n",
      "        -7.2777e-01, -3.8607e-02, -9.0719e-01, -3.0985e-02,  3.0006e-01,\n",
      "        -7.2197e-02, -5.1841e-01,  2.8961e-01, -5.0777e-01, -2.1630e-01,\n",
      "         7.7996e-01, -3.9773e-01,  1.1963e-01,  9.7837e-02,  1.9423e-01,\n",
      "        -1.1619e-01,  5.1299e-01,  3.9012e-01,  1.9648e-01,  5.5727e-01,\n",
      "        -1.8866e-01,  2.5906e-01,  3.1773e-01,  2.3591e-01,  4.7481e-01,\n",
      "         2.4647e-01,  3.9816e-01, -1.2829e-01,  2.1693e-01,  3.0199e-01,\n",
      "        -5.1283e-01,  1.7763e-01,  9.2063e-01, -8.0538e-01, -3.8126e-01,\n",
      "         8.7854e-01,  3.1959e-01, -1.1887e+00,  3.8662e-02,  3.5731e-01,\n",
      "        -5.3912e-01, -1.9993e-01,  3.6214e-01, -9.5700e-03, -1.5577e-01,\n",
      "         8.4230e-01,  3.1727e-01,  1.6622e-01,  5.0477e-01, -6.5845e-01,\n",
      "        -5.9936e-02,  2.5454e-01,  1.7821e-01, -9.3864e-02,  4.8066e-01,\n",
      "        -3.1617e-01,  2.8799e-01, -1.5255e-01, -6.0979e-02,  8.6163e-03,\n",
      "         3.2690e-01,  2.4095e-01,  2.2764e-01,  1.7180e-02,  2.0999e-01,\n",
      "         7.7600e-01,  6.8214e-01,  1.9866e-01,  4.4176e-01, -2.2065e-01,\n",
      "        -1.8478e-01,  6.3776e-01,  3.0860e-01,  2.8912e-01,  4.1345e-01,\n",
      "        -1.5780e-02,  3.1374e-03,  4.0891e-01,  1.9191e-01, -4.7082e-02,\n",
      "         1.9593e-01, -1.8441e-01,  6.0245e-01,  6.6626e-01,  1.5092e-01,\n",
      "         1.1175e-01, -4.7923e-01, -8.6853e-02,  7.9475e-02,  2.2919e-01,\n",
      "        -2.9255e-01, -3.3479e-01,  1.3469e-01, -3.3457e-01,  7.1964e-01,\n",
      "         5.2022e-01,  1.1658e-01, -5.0640e-03,  4.6689e-01, -8.2585e-02,\n",
      "        -3.9288e-01, -3.0215e-01, -1.3869e-01, -1.0991e-01, -7.3397e-02,\n",
      "        -9.7771e-01, -2.8629e-01, -3.2322e-01, -6.8916e-01, -2.4891e-01,\n",
      "        -3.6995e-01, -2.1346e-01, -3.6957e-01,  3.8878e-01, -7.2377e-02,\n",
      "        -2.1841e-01, -2.4810e-01,  2.2017e-01, -1.1896e-01,  6.4719e-01,\n",
      "         5.9376e-01,  6.3823e-01, -1.4502e-01,  1.7885e-01, -1.7623e-01,\n",
      "        -1.6338e-01,  3.4370e-01, -9.6935e-02,  1.4116e-01, -8.7434e-02,\n",
      "        -8.1225e-02, -6.5438e-03, -5.0890e-01,  2.3219e-02,  3.4893e-01,\n",
      "        -8.1887e-01,  1.6746e-01, -6.3583e-01,  5.3399e-03,  3.4883e-01,\n",
      "        -2.4093e-02, -5.1295e-02,  3.7916e-01, -2.6181e-01, -5.2637e-01,\n",
      "        -2.6681e-02,  4.1863e-01, -1.1506e-01, -1.3230e-01,  1.8941e-01,\n",
      "         7.6028e-01, -2.9705e-01, -4.8383e-01, -1.4199e-01,  1.8376e-01,\n",
      "        -2.3334e-01,  2.0482e-01,  2.1262e-01, -2.3749e-01, -2.9009e-01,\n",
      "        -5.1003e-02, -3.5639e-01, -4.0039e-01,  6.2812e-01, -4.7107e-02,\n",
      "         2.2477e-01, -3.4145e-01,  1.2932e-01,  3.7428e-01, -2.1998e-01,\n",
      "        -4.1132e-01, -7.7104e-03,  3.2543e-01, -3.9873e-01, -1.2895e-01,\n",
      "        -1.2249e-02, -7.0725e-02, -3.8953e-02,  3.3862e-01,  8.3120e-02,\n",
      "        -4.5823e-01, -3.3362e-01, -1.3105e-01])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the embeddings for the sample sentences\n",
    "for i, sentence in enumerate(sample_sentences):\n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    print(f\"Embedding: {sample_embeddings[i]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c622d50c",
   "metadata": {},
   "source": [
    "# Task 2: Multi-Task Learning Expansion\n",
    "\n",
    "\n",
    "Expand the sentence transformer to handle multi-task learning for:\n",
    "\n",
    "**Task A: Sentence Classification** – Classify sentences into predefined classes.<br/>\n",
    "**Task B: Sentiment Analysis** – Classify sentences into different sentiment categories.(I have chosen Sentiment Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685de9fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0fdf13fa",
   "metadata": {},
   "source": [
    "## Define the Multi-Task Model Architecture\n",
    "\n",
    "Add task-specific heads to the transformer model.Ensuring the transformer backbone is shared.\n",
    "\n",
    "\n",
    "## Implemented the Multi-Task Model\n",
    "\n",
    "Defined a custom model class. Implemented a forward pass to handle both tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a39d259c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiTaskModel(\n",
       "  (transformer): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (classification_head): Linear(in_features=768, out_features=5, bias=True)\n",
       "  (sentiment_head): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "\n",
    "class MultiTaskModel(nn.Module):\n",
    "    def __init__(self, transformer, num_classes, num_sentiments):\n",
    "        super(MultiTaskModel, self).__init__()\n",
    "        self.transformer = transformer\n",
    "        self.classification_head = nn.Linear(transformer.config.hidden_size, num_classes)\n",
    "        self.sentiment_head = nn.Linear(transformer.config.hidden_size, num_sentiments)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, task='classification'):\n",
    "        # Pass inputs through the transformer model\n",
    "        outputs = self.transformer(input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Mean pooling to get a single vector representation\n",
    "        pooled_output = outputs.last_hidden_state.mean(dim=1)\n",
    "        \n",
    "        # Return task-specific output\n",
    "        if task == 'classification':\n",
    "            return self.classification_head(pooled_output)\n",
    "        elif task == 'sentiment':\n",
    "            return self.sentiment_head(pooled_output)\n",
    "\n",
    "# Initialize the model\n",
    "num_classes = 5  # Example number of classes for classification\n",
    "num_sentiments = 3  # Example number of sentiment categories\n",
    "\n",
    "# Load the pre-trained transformer model\n",
    "transformer_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "multi_task_model = MultiTaskModel(transformer_model, num_classes, num_sentiments)\n",
    "multi_task_model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3a6b0c",
   "metadata": {},
   "source": [
    "## Preparing Data for Both Tasks\n",
    "\n",
    "Create sample datasets for both sentence classification and sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e77606a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data for sentence classification\n",
    "classification_sentences = [\"I love painting.\", \"The movie was pathetic.\", \"What a beautiful day!\", \"I hate waiting in lines.\", \"The book was fascinating.\"]\n",
    "classification_labels = [1, 0, 1, 0, 1]  # Example labels (e.g., 1 for positive, 0 for negative)\n",
    "\n",
    "# Sample data for sentiment analysis\n",
    "sentiment_sentences = [\"This product is amazing!\", \"I'm very disappointed.\", \"It's okay, not great.\", \"Absolutely fantastic!\", \"Really bad experience.\"]\n",
    "sentiment_labels = [2, 0, 1, 2, 0]  # Example sentiment labels (e.g., 2 for positive, 1 for neutral, 0 for negative)\n",
    "\n",
    "# Tokenize the sentences\n",
    "classification_inputs = tokenizer(classification_sentences, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "sentiment_inputs = tokenizer(sentiment_sentences, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "\n",
    "# Move inputs to the appropriate device\n",
    "classification_input_ids = classification_inputs['input_ids'].to(device)\n",
    "classification_attention_mask = classification_inputs['attention_mask'].to(device)\n",
    "sentiment_input_ids = sentiment_inputs['input_ids'].to(device)\n",
    "sentiment_attention_mask = sentiment_inputs['attention_mask'].to(device)\n",
    "\n",
    "# Convert labels to tensors\n",
    "classification_labels = torch.tensor(classification_labels).to(device)\n",
    "sentiment_labels = torch.tensor(sentiment_labels).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debc512f",
   "metadata": {},
   "source": [
    "## Test the Multi-Task Model\n",
    "We'll test the multi-task model with the sample data and verify the outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4df6be48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Outputs:\n",
      "tensor([[ 0.3300,  0.0477, -0.1783,  0.2054, -0.2476],\n",
      "        [ 0.0081, -0.0287, -0.0305,  0.0980, -0.3529],\n",
      "        [ 0.3278, -0.2119, -0.1304,  0.0764, -0.4244],\n",
      "        [ 0.1804,  0.0248,  0.0760,  0.2767, -0.2558],\n",
      "        [ 0.0069, -0.1111, -0.0364,  0.0935, -0.4658]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Forward pass for classification task\n",
    "classification_outputs = multi_task_model(classification_input_ids, classification_attention_mask, task='classification')\n",
    "print(\"Classification Outputs:\")\n",
    "print(classification_outputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b42ba8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Analysis Outputs:\n",
      "tensor([[ 0.2429, -0.2338,  0.0498],\n",
      "        [-0.1002, -0.0045, -0.2453],\n",
      "        [ 0.1496, -0.4888, -0.1407],\n",
      "        [ 0.1473, -0.1358,  0.0823],\n",
      "        [ 0.0780, -0.2310, -0.1272]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Forward pass for sentiment analysis task\n",
    "sentiment_outputs = multi_task_model(sentiment_input_ids, sentiment_attention_mask, task='sentiment')\n",
    "print(\"Sentiment Analysis Outputs:\")\n",
    "print(sentiment_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff6bd38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "288456e3",
   "metadata": {},
   "source": [
    "# Task 3: Training Considerations\n",
    "##### Discuss the implications and advantages of each scenario and explain your rationale as to how the model should be trained given the following:<br/>\n",
    "If the entire network should be frozen.<br/>\n",
    "If only the transformer backbone should be frozen.<br/>\n",
    "If only one of the task-specific heads (either for Task A or Task B) should be frozen.<br/><br/>\n",
    "##### Consider a scenario where transfer learning can be beneficial. Explain how you would approach the transfer learning process, including:<br/>\n",
    "The choice of a pre-trained model.<br/>\n",
    "The layers you would freeze/unfreeze.<br/>\n",
    "The rationale behind these choices.<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1054cc7",
   "metadata": {},
   "source": [
    "##  Example Implementation for Transfer Learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d0f726f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertModel, BertTokenizer, AdamW\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf92890",
   "metadata": {},
   "source": [
    "### Set Up the Environment and Model by defining the Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "33d5a563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiTaskModel(\n",
       "  (transformer): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (classification_head): Linear(in_features=768, out_features=5, bias=True)\n",
       "  (sentiment_head): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the model, tokenizer, and device\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "transformer_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "num_classes = 5  # Example number of classes for classification\n",
    "num_sentiments = 3  # Example number of sentiment categories\n",
    "\n",
    "class MultiTaskModel(nn.Module):\n",
    "    def __init__(self, transformer, num_classes, num_sentiments):\n",
    "        super(MultiTaskModel, self).__init__()\n",
    "        self.transformer = transformer\n",
    "        self.classification_head = nn.Linear(transformer.config.hidden_size, num_classes)\n",
    "        self.sentiment_head = nn.Linear(transformer.config.hidden_size, num_sentiments)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, task='classification'):\n",
    "        outputs = self.transformer(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state.mean(dim=1)\n",
    "        if task == 'classification':\n",
    "            return self.classification_head(pooled_output)\n",
    "        elif task == 'sentiment':\n",
    "            return self.sentiment_head(pooled_output)\n",
    "\n",
    "multi_task_model = MultiTaskModel(transformer_model, num_classes, num_sentiments)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "multi_task_model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cdaf8d",
   "metadata": {},
   "source": [
    "## Sample Data and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f2b59cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data for sentence classification\n",
    "classification_sentences = [\"I love painting.\", \"The movie was pathetic.\", \"What a beautiful day!\", \"I hate waiting in lines.\", \"The book was fascinating.\"]\n",
    "classification_labels = [1, 0, 1, 0, 1]\n",
    "\n",
    "# Sample data for sentiment analysis\n",
    "sentiment_sentences = [\"This product is amazing!\", \"I'm very disappointed.\", \"It's okay, not great.\", \"Absolutely fantastic!\", \"Really bad experience.\"]\n",
    "sentiment_labels = [2, 0, 1, 2, 0]\n",
    "\n",
    "# Tokenize the sentences\n",
    "classification_inputs = tokenizer(classification_sentences, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "sentiment_inputs = tokenizer(sentiment_sentences, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "\n",
    "# Convert inputs and labels to tensors\n",
    "classification_input_ids = classification_inputs['input_ids']\n",
    "classification_attention_mask = classification_inputs['attention_mask']\n",
    "classification_labels = torch.tensor(classification_labels)\n",
    "\n",
    "sentiment_input_ids = sentiment_inputs['input_ids']\n",
    "sentiment_attention_mask = sentiment_inputs['attention_mask']\n",
    "sentiment_labels = torch.tensor(sentiment_labels)\n",
    "\n",
    "# Create TensorDatasets and DataLoaders\n",
    "classification_dataset = TensorDataset(classification_input_ids, classification_attention_mask, classification_labels)\n",
    "sentiment_dataset = TensorDataset(sentiment_input_ids, sentiment_attention_mask, sentiment_labels)\n",
    "\n",
    "classification_loader = DataLoader(classification_dataset, batch_size=2, shuffle=True)\n",
    "sentiment_loader = DataLoader(sentiment_dataset, batch_size=2, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d540b95a",
   "metadata": {},
   "source": [
    "## Defining the Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ec1b0a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaina\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial training for task-specific heads completed.\n",
      "Fine-tuning completed.\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Define training parameters\n",
    "num_epochs = 3  # Number of epochs\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Define loss functions\n",
    "criterion_classification = nn.CrossEntropyLoss()\n",
    "criterion_sentiment = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(multi_task_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop for task-specific heads\n",
    "multi_task_model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training for classification task\n",
    "    for input_ids, attention_mask, labels in classification_loader:\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = multi_task_model(input_ids, attention_mask, task='classification')\n",
    "        loss = criterion_classification(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Training for sentiment analysis task\n",
    "    for input_ids, attention_mask, labels in sentiment_loader:\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = multi_task_model(input_ids, attention_mask, task='sentiment')\n",
    "        loss = criterion_sentiment(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "print(\"Initial training for task-specific heads completed.\")\n",
    "\n",
    "# Unfreeze lower layers for further fine-tuning\n",
    "for layer in multi_task_model.transformer.encoder.layer[:4]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "# Lower the learning rate for fine-tuning\n",
    "optimizer = AdamW(multi_task_model.parameters(), lr=1e-5)\n",
    "\n",
    "# Fine-tuning loop\n",
    "multi_task_model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Fine-tuning for classification task\n",
    "    for input_ids, attention_mask, labels in classification_loader:\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = multi_task_model(input_ids, attention_mask, task='classification')\n",
    "        loss = criterion_classification(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Fine-tuning for sentiment analysis task\n",
    "    for input_ids, attention_mask, labels in sentiment_loader:\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = multi_task_model(input_ids, attention_mask, task='sentiment')\n",
    "        loss = criterion_sentiment(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "print(\"Fine-tuning completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a2ede5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a367250",
   "metadata": {},
   "source": [
    "## Task 4: Layer-wise Learning Rate Implementation (BONUS) \n",
    "\n",
    "\n",
    "Implement layer-wise learning rates for the multi-task sentence transformer.<br/>\n",
    "Explain the rationale for the specific learning rates you've set for each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73132a24",
   "metadata": {},
   "source": [
    "# Define Layer-wise Learning Rates\n",
    "Layer-wise learning rates allow us to assign different learning rates to different parts of the model. Typically, lower layers (closer to the input) receive lower learning rates, while higher layers (closer to the output) receive higher learning rates. This is because lower layers capture more general features that should remain relatively stable, while higher layers capture task-specific features that might need more adaptation.\n",
    "\n",
    "Layer-wise Learning Rate Configuration: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f417fc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define different learning rates for different layers\n",
    "learning_rate_base = 1e-5\n",
    "layer_learning_rates = {\n",
    "    'transformer.encoder.layer.0': learning_rate_base,\n",
    "    'transformer.encoder.layer.1': learning_rate_base,\n",
    "    'transformer.encoder.layer.2': learning_rate_base,\n",
    "    'transformer.encoder.layer.3': learning_rate_base,\n",
    "    'transformer.encoder.layer.4': learning_rate_base * 2,\n",
    "    'transformer.encoder.layer.5': learning_rate_base * 2,\n",
    "    'transformer.encoder.layer.6': learning_rate_base * 2,\n",
    "    'transformer.encoder.layer.7': learning_rate_base * 2,\n",
    "    'transformer.encoder.layer.8': learning_rate_base * 3,\n",
    "    'transformer.encoder.layer.9': learning_rate_base * 3,\n",
    "    'transformer.encoder.layer.10': learning_rate_base * 3,\n",
    "    'transformer.encoder.layer.11': learning_rate_base * 3,\n",
    "    'classification_head': learning_rate_base * 5,\n",
    "    'sentiment_head': learning_rate_base * 5,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e4433a",
   "metadata": {},
   "source": [
    "## Modify the Optimizer to Use Layer-wise Learning Rates\n",
    "To implement layer-wise learning rates, we need to group the model parameters by layer and assign the specified learning rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c8e926d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layerwise_lr_params(model, layer_learning_rates):\n",
    "    params = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if not param.requires_grad:\n",
    "            continue\n",
    "        lr = None\n",
    "        for layer_name in layer_learning_rates:\n",
    "            if name.startswith(layer_name):\n",
    "                lr = layer_learning_rates[layer_name]\n",
    "                break\n",
    "        if lr is None:\n",
    "            lr = learning_rate_base\n",
    "        params.append({'params': param, 'lr': lr})\n",
    "    return params\n",
    "\n",
    "# Create parameter groups with specified learning rates\n",
    "layerwise_params = get_layerwise_lr_params(multi_task_model, layer_learning_rates)\n",
    "optimizer = AdamW(layerwise_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e200675",
   "metadata": {},
   "source": [
    "## Training Loop with Layer-wise Learning Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "76298bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with layer-wise learning rates completed.\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 3  # Define number of epochs\n",
    "\n",
    "# Training loop with layer-wise learning rates\n",
    "multi_task_model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training for classification task\n",
    "    for input_ids, attention_mask, labels in classification_loader:\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = multi_task_model(input_ids, attention_mask, task='classification')\n",
    "        loss = criterion_classification(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Training for sentiment analysis task\n",
    "    for input_ids, attention_mask, labels in sentiment_loader:\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = multi_task_model(input_ids, attention_mask, task='sentiment')\n",
    "        loss = criterion_sentiment(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "print(\"Training with layer-wise learning rates completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e472a56",
   "metadata": {},
   "source": [
    "# Rationale:\n",
    "\n",
    "**Lower Layers (0-3)**: Lower learning rates (1x base) to prevent overfitting. These layers capture general linguistic features that are broadly applicable and should remain stable.<br/>\n",
    "**Middle Layers (4-7)**: Slightly higher learning rates (2x base) as these layers start to capture more task-specific information that may need more fine-tuning.<br/>\n",
    "**Upper Layers (8-11)**: Higher learning rates (3x base) as these layers capture the most task-specific features and benefit from more adaptation to the new tasks.<br/>\n",
    "**Task-specific Heads**: Highest learning rates (5x base) because these layers are entirely new and need significant tuning to learn the task-specific mappings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea598094",
   "metadata": {},
   "source": [
    "# Discuss the Potential Benefits of Layer-wise Learning Rates\n",
    "\n",
    "\n",
    "**Benefits:**\n",
    "\n",
    "**Better Fine-Tuning:** Allows more fine-tuned adjustments to the model, improving performance by adapting different parts of the network at appropriate rates.<br/>\n",
    "**Prevent Overfitting:** Helps prevent overfitting in lower layers while allowing higher layers to adapt more readily to task-specific nuances.<br/>\n",
    "**Efficiency:** Optimizes the training process by focusing learning where it's most needed, leading to potentially faster convergence and better overall performance.<br/>\n",
    "**Multi-Task Setting:** In a multi-task setting, layer-wise learning rates ensure that the shared layers (transformer backbone) adapt properly while the task-specific heads receive enough focus to learn the distinct tasks effectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12143410",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8ef5dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14529bed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcd5698",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
